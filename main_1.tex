%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[11pt, twosides]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
%   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 419M Introduction to Machine Learning
                        \hfill Spring 2021-22} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
% \renewcommand{\cite}[1]{[#1]}
% \def\beginrefs{\begin{list}%
%         {[\arabic{equation}]}{\usecounter{equation}
%          \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%          \setlength{\labelwidth}{1.6truecm}}}
% \def\endrefs{\end{list}}
% \def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
% \newcommand{\fig}[3]{
% 			\vspace{#2}
% 			\begin{center}
% 			Figure \thelecnum.#1:~#3
% 			\end{center}
% 	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
% \newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{15}{Clustering}{Abir De}{Group 2}
%\lecture{x}{Title}{Abir De}{Group y}

\section{Introduction to Clustering}

\begin{enumerate}
    \item Clustering is an unsupervised classification technique which involves grouping of values in a given data set close to the representative value.
    \item A typical problem involving clustering might be a case where we are given a set of points in R or N and we need to identify clusters
    \item Active learning, on the other hand, is supervised classification technique.
    \item For instance, if we have a set given in which the tags are not known, then we make use of clustering to find those tags. The label for the representative value of a cluster would be the label for the entire cluster.
\end{enumerate}\\
\textbf{Clustering is unsupervised still it helps in supervision.} This is done by ensuring that it reduces the number of labels we need to find.\\
Let us assume a data set (x,y) and let f$_{\theta}$(x) represent the map from x to y with parameters $\theta$:\\~\\
\centerline{(x,y) $\xrightarrow$ f$_{\theta}$(x) $\xrightarrow{train}$  f$_\hat{\theta}$(x)}\\~\\
For example, we might need to search for a query among a large number of results. Let x$_c$ represent the corpus data and x$_q$ represent the query. In such a case, it is not a good idea to search over the entire corpus data. Alternatively, we can cluster x$_c$ to find the best mean representative to x$_q$. Thus, it helps in restricting the run search space. Let us consider a function f$_\theta$, which says how relevant a corpus is to the query,\\~\\
\centerline{f$_\theta$(x$_q$,x$_c$) = +1 implies c will be returned to query q}\\
\centerline{f$_\theta$(x$_q$,x$_c$) = -1 implies c will not be returned to query q}\\~\\
where c $\in$ e. \\~\\
So the process of clustering e into k cluster involves:
\begin{enumerate}
    \item Cluster e into k groups
    \item Provide cluster IDs
    \item Map the query to cluster ID i
    \item Compute f$_\hat{\theta}$(x$_q$,x$_c$) for all c in cluster i
    \item Return relevant and non-relevant corpus items
\end{enumerate}
\\
Now we introduce the notion of a similarity function with cosine similarity as an example. Let h(x$_q$) and h(x$_c$) be the suitable vector representation to each data point x$_q$ and x$_c$ respectively.\\~\\
Define the cosine similarity between the two as: 
cos(h(x$_q$),h(x$_c$)) $:=\frac{h(x_q)}{\|h(x_q)\|}.\frac{h(x_c)}{\|h(x_c)\|}$\\~\\
This gives a measure of the degree of closeness between the two vectors $h(x_c)$ and $h(x_q)$. Now consider the following which can make more rigorous the "similarity" aspect of the cosine similarity $s$:\\
Let u $\sim$ N(0,I) be a random vector.
For each vector $h(x_c)$ pick a representative vector as: Rep(h(x$_c$)) = sgn[$\frac{u}{||u||}\odot $h(x$_c$)]. These representative vectors can be said to divide this space of dimension $d$ into $2^d$ clusters. We can find suitable bounds: \\~\\
If s(h(x$_q$), h(x$_c$)) $>$ R  then P(Rep(h(x$_q$)) = Rep(h(x$_c$))) $\geq$ $\epsilon$ \\
If s(h(x$_q$), h(x$_c$)) $<$ R/a (where a$>$1)  then P(Rep(h(x$_q$)) = Rep(h(x$_c$))) $\leq$ $\epsilon$$_{1}$\\
where $\epsilon$ $>$$>$ $\epsilon_{1}$.\\
This indicates that $s$ indeed behaves like a "similarity function".

\section{Similarity and Difference Metrics}

Given 2 vectors, $x_1$ and $x_2$, we have looked at the cosine similarity function

\begin{equation}
    s = cos(x_1,x_2) = \frac{x_1}{\|x_1\|}.\frac{x_2}{\|x_2\|}
\end{equation}

We can thus define $v_1 = \frac{x_1}{\|x_1\|}$ and $v_2 = \frac{x_2}{\|x_2\|}$ so that

\begin{equation}
    s = v_1.v_2 \;\;\; and \;\;\; \|v_1\| = \|v_2\| = 1
\end{equation}

We can further define an intuitive distance function, $\Delta$ for $v_1$ and $v_2$ only in terms of $s$ in the following way

\begin{equation}
    \Delta = \sqrt{2-2s} = \sqrt{(v_1-v_2)^2} = \sqrt{\left(\frac{x_1}{\|x_1\|}-\frac{x_2}{\|x_2\|}\right)^2}
\end{equation}

But what if we do not know the norm of both the vectors? How do we proceed then?\\

Let us take an example. We would like to convert $\frac{x_1}{\|x_1\|}.x_2$ to a cosine similarity under the condition that $\|x_2\|<B$. Therefore, we need to find $v_1$ and $v_2$ such that 

\begin{equation}
    v_1.v_2 = \frac{x_1}{\|x_1\|}.x_2 \; \; \; and \; \; \; \|v_1\| = \|v_2\| = 1
\end{equation}

We can do this by increasing the dimensionality by 1 and defining $v_1$ and $v_2$ in the following manner

\begin{equation}
    v_1 = \begin{bmatrix}
        \frac{x_1}{\|x_1\|} \\
        0
    \end{bmatrix} 
    \;\;\; and \;\;\;
    v_2 = \begin{bmatrix}
         x_2  \\
        \;\sqrt{B^2-x_2^2}\;
    \end{bmatrix} 
\end{equation}

We can easily verify that

\begin{equation}
    v_1.v_2 = \frac{x_1}{\|x_1\|}.x_2 \; \; \; and \; \; \; \|v_1\| = \|v_2\| = 1
\end{equation}

We can extend this idea to convert $x_1.x_2$ to a cosine similarity under the condition that $\|x_1\|<B_1$ and $\|x_2\|<B_2$ by defining $v_1$ and $v_2$ in the following manner

\begin{equation}
    v_1 = \begin{bmatrix}
         x_1  \\
         0 \\
        \;\sqrt{B_1^2-x_1^2}\;
    \end{bmatrix} 
    \;\;\; and \;\;\;
    v_2 = \begin{bmatrix}
         x_2  \\
        \;\sqrt{B_2^2-x_2^2}\; \\
        0
    \end{bmatrix} 
\end{equation}\\

Now, let us look at Difference metrics for Sets.\\

Given 2 sets, $S_1 = \{x_1, x_2, ..., x_n\}$ and $S_2 = \{x_1', x_2', ..., x_n'\}$, we define the following distance function:

\begin{equation}
    \Delta (S_1,S_2) = 1 - \frac{|S_1 \cap S_2|}{|S_1 \cup S_2|}
\end{equation}

Verify whether this function is indeed a distance metric.\\

Note that for a distance function, $\Delta$ to be a distance metric, it must satisfy the following 3 conditions:

\begin{equation}
    \Delta(S,S) = 0
\end{equation}

\begin{equation}
    \Delta(S_1,S_2) \geq 0 \;\;\; and \;\;\; \Delta(S_1,S_2) = 0 \implies S_1 = S_2
\end{equation}

\begin{equation}
    \Delta(S_1,S_3) + \Delta(S_2,S_3) \geq \Delta(S_1,S_2)
\end{equation}\\

Hint: The distance function given above is indeed a distance metric (called the Jaccard Distance). For proving that it satisfies the 3 conditions, one can look up the Steinhaus Transform.

\section{K means Clustering}
Suppose we are given a dataset $D=\{x_1,x_2,..,x_n\}$ (here suppose the elements are vectors). Our aim is to split this data into K classes/clusters $C_1, C_2,..., C_K$ such that points in each cluster are "closest" to their own cluster. We can formally say that we want to solve the following optimisation problem. Lets call it {\bf Problem A}:  
\begin{center}
    min $_{C_1,..,C_K}\sum_{k=1}^K\sum_{i,j\in C_k} \|x_i-x_j\|^2 $\\
    with: $D=\bigcup_{i=1}^KC_i$ , $C_i\cap C_j=\emptyset \  \forall \  i\neq j$
\end{center}
However, it is more convenient for us to solve the following problem. Lets call it {\bf Problem B}:

\begin{center}
    min $_{\{ \Bar{x}_k\}, \{C_k\}}\sum_{k=1}^K\sum_{i\in C_k} \|x_i-\Bar{x}_k\|^2 $\\
    with: $D=\bigcup_{i=1}^KC_i$ , $C_i\cap C_j=\emptyset \  \forall \  i\neq j$
\end{center}
Note that to minimise the objective function, we need to choose $\Bar{x}_k$ as the mean of cluster $C_k$, so we can say:  $\Bar{x}_k = \frac{1}{|C_k|}\sum_{i\in C_k} x_i $. \\

We can rewrite the above two problems using a collection of weights $p_{ik}$, which indicate the cluster each element belongs to.\\
Here $p_{ik} = 1$ iff $i \in C_k$ and 0 otherwise:
\begin{center}
    {\bf Problem A:} min $_{ \{p_{ik}\} } \sum_{k=1}^K\sum_{i,j\in D} p_{ik}\  p_{jk}\ \|x_i-x_j\|^2 $\\
    with: $\sum_{k=1}^K p_{ik} = 1$
\end{center}

\begin{center}
    {\bf Problem B:} min $_{\{ \Bar{x}_k\}, \{p_{ik}\} } \sum_{k=1}^K\sum_{i\in D} p_{ik} \|x_i-\Bar{x}_k\|^2 $\\
    with: $\sum_{k=1}^K p_{ik} = 1$
\end{center}

This just rewrites the optimisation problem w.r.t. the classes $C_k$ as an optimisation w.r.t. a set of coefficients $p_{ik}$.\\

Now let us try and somewhat relate the problems A and B using simple algebra. Consider a set $C$ of size $|C|$ (C can be any cluster $C_k$), containing elements $x_1, x_2,..,x_{|C|}$ (taken to be vectors). Let $\Bar{x}:=\frac{1}{|C|}\sum_{i\in C} x_i$ be the mean of the elements. We have:
\begin{equation}
    \sum_{i\in C}  \|x_i-\Bar{x}\|^2 = \sum_{i\in C}  \|x_i-\frac{1}{|C|}\sum_{j\in C} x_j\|^2
\end{equation}

\begin{equation}
    =\frac{1}{|C|^2}\sum_{i\in C}\|\sum_{j\in C}(x_i-x_j)\|^2=\frac{1}{|C|^2}\sum_{i\in C}(\sum_{j\in C}(x_i-x_j)^T \sum_{k\in C}(x_i-x_k))
\end{equation}

\begin{equation}
    =\frac{1}{|C|^2}\sum_{i,j,k\in C}(x_i-x_j)^T(x_i-x_k)
\end{equation}
\begin{equation}
    =\frac{1}{|C|^2}\sum_{i,j,k\in C}(x_i-x_j)^T(x_i-x_j)+\frac{1}{|C|^2}\sum_{i,j,k\in C}(x_i-x_j)^T(x_j-x_k)
\end{equation}
\begin{equation}
    =\frac{1}{|C|}\sum_{i,j\in C}\|x_i-x_j\|^2+\frac{1}{|C|}\sum_{i,j\in C}(x_i-x_j)^T(x_j-\Bar{x})
\end{equation}
\begin{equation}
    =\frac{1}{|C|}\sum_{i,j\in C}\|x_i-x_j\|^2+\sum_{j\in C}(\Bar{x}-x_j)^T(x_j-\Bar{x})
\end{equation}
\begin{equation}
    =\frac{1}{|C|}\sum_{i,j\in C}\|x_i-x_j\|^2-\sum_{j\in C}\|x_j-\Bar{x}\|^2
\end{equation}
On rearranging the final equivalence, we obtain:
\begin{equation}
    \sum_{i\in C}\|x_i-\Bar{x}\|^2=\frac{1}{2|C|}\sum_{i,j\in C}\|x_i-x_j\|^2
\end{equation}
From this result, we cannot deduce the equivalence of the two problems. This is because each cluster size $|C_k|$ may be different and so due to the $2|C_k|$ factor in the above result, the optimisation functions are not related by a constant factor.

\section{Group Details and Individual Contribution}

The names, roll numbers and contributions of the group members who worked on this scribe are mentioned below:

\begin{itemize}
    \item Raghav Rander (200040113) - Section 15.1
    \item Devak Sinha (180070017) - Section 15.2
    \item Waqar Mirza (200070090) - Section 15.3
\end{itemize}

\end{document}





